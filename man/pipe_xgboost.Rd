% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pipe_xgboost.R
\name{pipe_xgboost}
\alias{pipe_xgboost}
\title{eXtrem Gradient Boosted models}
\usage{
pipe_xgboost(
  df,
  predInput = NULL,
  responseVars = 1,
  caseClass = NULL,
  idVars = character(),
  weight = "class",
  crossValStrategy = c("Kfold", "bootstrap"),
  k = 5,
  replicates = 10,
  crossValRatio = c(train = 0.6, test = 0.2, validate = 0.2),
  params = list(),
  nrounds = 5,
  shap = TRUE,
  aggregate_shap = TRUE,
  repVi = 5,
  summarizePred = TRUE,
  scaleDataset = FALSE,
  XGBmodel = FALSE,
  DALEXexplainer = FALSE,
  variableResponse = FALSE,
  save_validateset = FALSE,
  baseFilenameXDG = NULL,
  filenameRasterPred = NULL,
  tempdirRaster = NULL,
  nCoresRaster = parallel::detectCores()\%/\%2,
  verbose = 0,
  ...
)
}
\arguments{
\item{df}{a \code{data.frame} with the data.}

\item{predInput}{a \code{data.frame} or a \code{Raster} with the input variables for the model as columns or layers. The columns or layer names must match the names of \code{df} columns.}

\item{responseVars}{response variables as column names or indexes on \code{df}.}

\item{caseClass}{class of the samples used to weight cases. Column names or indexes on \code{df}, or a vector with the class for each rows in \code{df}.}

\item{idVars}{id column names or indexes on \code{df}. This columns will not be used for training.}

\item{weight}{Optional array of the same length as \code{nrow(df)}, containing weights to apply to the model's loss for each sample.}

\item{crossValStrategy}{\code{Kfold} or \code{bootstrap}.}

\item{k}{number of data partitions when \code{crossValStrategy="Kfold"}.}

\item{replicates}{number of replicates for \code{crossValStrategy="bootstrap"} and \code{crossValStrategy="Kfold"} (\code{replicates * k-1}, 1 fold for validation).}

\item{crossValRatio}{proportion of the dataset used to train, test and validate the model when \code{crossValStrategy="bootstrap"}. Default to \code{c(train=0.6, test=0.2, validate=0.2)}. If there is only one value, will be taken as a train proportion and the test set will be used for validation.}

\item{params}{the list of parameters to \code{\link[xgboost:xgb.train]{xgboost::xgb.train()}}. The complete list of parameters is available in the \href{https://xgboost.readthedocs.io/en/latest/parameter.html}{online documentation}.}

\item{nrounds}{max number of boosting iterations.}

\item{shap}{if \code{TRUE}, return the SHAP values as \code{\link[shapviz:shapviz]{shapviz::shapviz()}} objects.}

\item{aggregate_shap}{if \code{TRUE}, and \code{shap} is also \code{TRUE}, aggregate SHAP from all replicates.}

\item{repVi}{replicates of the permutations to calculate the importance of the variables. 0 to avoid calculating variable importance.}

\item{summarizePred}{if \code{TRUE}, return the mean, sd and se of the predictors. if \code{FALSE}, return the predictions for each replicate.}

\item{scaleDataset}{if \code{TRUE}, scale the whole dataset only once instead of the train set at each replicate. Optimize processing time for predictions with large rasters.}

\item{XGBmodel}{if \code{TRUE}, return the model with the result.}

\item{DALEXexplainer}{if \code{TRUE}, return a explainer for the models from \code{\link[DALEX:explain]{DALEX::explain()}} function. It doesn't work with multisession future plans.}

\item{variableResponse}{if \code{TRUE}, return aggregated_profiles_explainer object from \code{\link[ingredients:partial_dependence]{ingredients::partial_dependency()}} and the coefficients of the adjusted linear model.}

\item{save_validateset}{save the validateset (independent data not used for training).}

\item{baseFilenameXDG}{if no missing, save the NN in hdf5 format on this path with iteration appended.}

\item{filenameRasterPred}{if no missing, save the predictions in a RasterBrick to this file.}

\item{tempdirRaster}{path to a directory to save temporal raster files.}

\item{nCoresRaster}{number of cores used for parallelized raster cores. Use half of the available cores by default.}

\item{verbose}{if > 0, print the state. The bigger the more information printed.}

\item{...}{extra parameters for \code{\link[xgboost:xgb.train]{xgboost::xgb.train()}}, \code{\link[future.apply:future_lapply]{future.apply::future_replicate()}} or \code{\link[ingredients:feature_importance]{ingredients::feature_importance()}}.}
}
\description{
eXtrem Gradient Boosted models
}
