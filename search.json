[{"path":"https://jmaspons.github.io/MLTools/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Joan Maspons. Author, maintainer. Helena Vallicrosa. Contributor.","code":""},{"path":"https://jmaspons.github.io/MLTools/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Maspons J (2023). NNTools: Pipes tools machine learning. R package version 0.0.19, https://jmaspons.github.io/MLTools/, https://jmaspons.github.io/MLTools.","code":"@Manual{,   title = {NNTools: Pipes and tools for machine learning},   author = {Joan Maspons},   year = {2023},   note = {R package version 0.0.19, https://jmaspons.github.io/MLTools/},   url = {https://jmaspons.github.io/MLTools}, }"},{"path":"https://jmaspons.github.io/MLTools/index.html","id":"nntools","dir":"","previous_headings":"","what":"Pipes and tools for machine learning","title":"Pipes and tools for machine learning","text":"goal NNTools facilitate use Machine Learning’ technics wrapping pipeline easy use functions (pipe_*). functions take care cross-validation, data scaling return predictions (also rasters), SHAP single call.","code":""},{"path":"https://jmaspons.github.io/MLTools/index.html","id":"pre-requisites","dir":"","previous_headings":"","what":"Pre-requisites","title":"Pipes and tools for machine learning","text":"Install keras tensorflow (needed pipe_keras pipe_keras_timeseries):","code":"keras::install_keras()"},{"path":"https://jmaspons.github.io/MLTools/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Pipes and tools for machine learning","text":"can install development version NNTools like :","code":"# install.packages(\"remotes\") remotes::install_github(\"jmaspons/MLTools\")  ## The package \"data.table\" (>= 1.14.9) is required. # data.table::update_dev_pkg() # requires unreleased features for reshaping 3D data"},{"path":"https://jmaspons.github.io/MLTools/reference/feature_importance.html","id":null,"dir":"Reference","previous_headings":"","what":"Feature Importance — feature_importance","title":"Feature Importance — feature_importance","text":"function calculates permutation based feature importance. reason also called Variable Dropout Plot.","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/feature_importance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance — feature_importance","text":"","code":"feature_importance(x, ...)  # S3 method for explainer feature_importance(   x,   loss_function = DALEX::loss_root_mean_square,   ...,   type = c(\"raw\", \"ratio\", \"difference\"),   n_sample = NULL,   B = 10,   variables = NULL,   variable_groups = NULL,   N = n_sample,   label = NULL )  # S3 method for default feature_importance(   x,   data,   y,   predict_function = predict,   loss_function = DALEX::loss_root_mean_square,   ...,   label = class(x)[1],   type = c(\"raw\", \"ratio\", \"difference\"),   n_sample = NULL,   B = 10,   variables = NULL,   N = n_sample,   variable_groups = NULL,   perm_dim = NULL,   comb_dims = FALSE )"},{"path":"https://jmaspons.github.io/MLTools/reference/feature_importance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance — feature_importance","text":"x explainer created function DALEX::explain(), model explained. ... parameters passed predict_function. loss_function function thet used assess variable importance. type character, type transformation applied dropout loss. \"raw\" results raw drop losses, \"ratio\" returns drop_loss / drop_loss_full_model \"difference\" returns drop_loss - drop_loss_full_model. n_sample alias N held backwards compatibility. number observations sampled calculation variable importance. B integer, number permutation rounds perform variable. default 10. variables vector variables list vectors multiinput models. NULL variable importance tested variable data separately. default NULL variable_groups list variables names vectors list vectors multiinput models. testing joint variable importance. NULL variable importance tested separately variables. default NULL. specified override variables,  perm_dim comb_dims. N number observations sampled calculation variable importance. NULL variable importance calculated whole dataset (sampling). label name model. default extracted class attribute model. data validation dataset, extracted x explainer. Can list arrays multiinput models. NOTE: safer target variable present data. y true labels data, extracted x explainer. predict_function predict function, extracted x explainer. perm_dim dimensions perform permutations data 3d array (e.g. [case, time, variable]). perm_dim = 2:3, calculates importance variable 2nd 3rd dimensions. multiinput models, list dimensions order  data.  NULL, default, take dimensions except first one (.e. rows) correspond cases. comb_dims TRUE, permutations combination levels variables 2nd 3rd dimensions input data 3 dimensions. default, FALSE.","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/feature_importance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Feature Importance — feature_importance","text":"object class feature_importance","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/feature_importance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Feature Importance — feature_importance","text":"Find details Feature Importance Chapter.","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/feature_importance.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Feature Importance — feature_importance","text":"Explanatory Model Analysis. Explore, Explain, Examine Predictive Models. https://ema.drwhy.ai/","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/feature_importance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Feature Importance — feature_importance","text":"","code":"library(\"DALEX\") #> Welcome to DALEX (version: 2.4.3). #> Find examples and detailed introduction at: http://ema.drwhy.ai/ #> Additional features will be available after installation of: ggpubr. #> Use 'install_dependencies()' to get all suggested dependencies #>  #> Attaching package: ‘DALEX’ #> The following object is masked from ‘package:NNTools’: #>  #>     feature_importance library(\"ingredients\") #>  #> Attaching package: ‘ingredients’ #> The following object is masked from ‘package:DALEX’: #>  #>     feature_importance #> The following object is masked from ‘package:NNTools’: #>  #>     feature_importance  model_titanic_glm <- glm(survived ~ gender + age + fare,                          data = titanic_imputed, family = \"binomial\")  explain_titanic_glm <- explain(model_titanic_glm,                                data = titanic_imputed[,-8],                                y = titanic_imputed[,8]) #> Preparation of a new explainer is initiated #>   -> model label       :  lm  (  default  ) #>   -> data              :  2207  rows  7  cols  #>   -> target variable   :  2207  values  #>   -> predict function  :  yhat.glm  will be used (  default  ) #>   -> predicted values  :  No value for predict function target column. (  default  ) #>   -> model_info        :  package stats , ver. 4.3.2 , task classification (  default  )  #>   -> predicted values  :  numerical, min =  0.1490412 , mean =  0.3221568 , max =  0.9878987   #>   -> residual function :  difference between y and yhat (  default  ) #>   -> residuals         :  numerical, min =  -0.8898433 , mean =  4.165191e-13 , max =  0.8448637   #>   A new explainer has been created!    fi_glm <- feature_importance(explain_titanic_glm, B = 1) plot(fi_glm)   if (FALSE) {  fi_glm_joint1 <- feature_importance(explain_titanic_glm,                    variable_groups = list(\"demographics\" = c(\"gender\", \"age\"),                    \"ticket_type\" = c(\"fare\")),                    label = \"lm 2 groups\")  plot(fi_glm_joint1)  fi_glm_joint2 <- feature_importance(explain_titanic_glm,                    variable_groups = list(\"demographics\" = c(\"gender\", \"age\"),                                           \"wealth\" = c(\"fare\", \"class\"),                                           \"family\" = c(\"sibsp\", \"parch\"),                                           \"embarked\" = \"embarked\"),                    label = \"lm 5 groups\")  plot(fi_glm_joint2, fi_glm_joint1)  library(\"ranger\") model_titanic_rf <- ranger(survived ~., data = titanic_imputed, probability = TRUE)  explain_titanic_rf <- explain(model_titanic_rf,                               data = titanic_imputed[,-8],                               y = titanic_imputed[,8],                               label = \"ranger forest\",                               verbose = FALSE)  fi_rf <- feature_importance(explain_titanic_rf) plot(fi_rf)  fi_rf <- feature_importance(explain_titanic_rf, B = 6) # 6 replications plot(fi_rf)  fi_rf_group <- feature_importance(explain_titanic_rf,                    variable_groups = list(\"demographics\" = c(\"gender\", \"age\"),                    \"wealth\" = c(\"fare\", \"class\"),                    \"family\" = c(\"sibsp\", \"parch\"),                    \"embarked\" = \"embarked\"),                    label = \"rf 4 groups\")  plot(fi_rf_group, fi_rf)  HR_rf_model <- ranger(status ~., data = HR, probability = TRUE)  explainer_rf  <- explain(HR_rf_model, data = HR, y = HR$status,                          model_info = list(type = 'multiclass'))  fi_rf <- feature_importance(explainer_rf, type = \"raw\",                             loss_function = DALEX::loss_cross_entropy) head(fi_rf) plot(fi_rf)  HR_glm_model <- glm(status == \"fired\"~., data = HR, family = \"binomial\") explainer_glm <- explain(HR_glm_model, data = HR, y = as.numeric(HR$status == \"fired\")) fi_glm <- feature_importance(explainer_glm, type = \"raw\",                              loss_function = DALEX::loss_root_mean_square) head(fi_glm) plot(fi_glm)  }"},{"path":"https://jmaspons.github.io/MLTools/reference/pipe_keras.html","id":null,"dir":"Reference","previous_headings":"","what":"Neural network model with keras — pipe_keras","title":"Neural network model with keras — pipe_keras","text":"Neural network model keras","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/pipe_keras.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Neural network model with keras — pipe_keras","text":"","code":"pipe_keras(   df,   predInput = NULL,   responseVars = 1,   caseClass = NULL,   idVars = character(),   weight = \"class\",   crossValStrategy = c(\"Kfold\", \"bootstrap\"),   k = 5,   replicates = 10,   crossValRatio = c(train = 0.6, test = 0.2, validate = 0.2),   hidden_shape = 50,   epochs = 500,   maskNA = NULL,   batch_size = \"all\",   shap = TRUE,   aggregate_shap = TRUE,   repVi = 5,   summarizePred = TRUE,   scaleDataset = FALSE,   NNmodel = FALSE,   DALEXexplainer = FALSE,   variableResponse = FALSE,   save_validateset = FALSE,   baseFilenameNN = NULL,   filenameRasterPred = NULL,   tempdirRaster = NULL,   nCoresRaster = parallel::detectCores()%/%2,   verbose = 0,   ... )"},{"path":"https://jmaspons.github.io/MLTools/reference/pipe_keras.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Neural network model with keras — pipe_keras","text":"df data.frame data. predInput data.frame Raster input variables model columns layers. columns layer names must match names df columns. responseVars response variables column names indexes df. caseClass class samples used weight cases. Column names indexes df, vector class rows df. idVars id column names indexes df. columns used training. weight Optional array length nrow(df), containing weights apply model's loss sample. crossValStrategy Kfold bootstrap. k number data partitions crossValStrategy=\"Kfold\". replicates number replicates crossValStrategy=\"bootstrap\" crossValStrategy=\"Kfold\" (replicates * k-1, 1 fold validation). crossValRatio Proportion dataset used train, test validate model crossValStrategy=\"bootstrap\". Default c(train=0.6, test=0.2, validate=0.2). one value, taken train proportion test set used validation. hidden_shape number neurons hidden layers neural network model. Can vector values hidden layer. epochs parameter keras::fit(). maskNA value assign NAs scaling passed keras::layer_masking(). batch_size fit predict functions. bigger better fits available memory. Integer \"\". shap TRUE, return SHAP values shapviz::shapviz() object (shapviz::mshapviz() multioutput models). aggregate_shap TRUE, shap also TRUE, aggregate SHAP replicates. repVi replicates permutations calculate importance variables. 0 avoid calculating variable importance. summarizePred TRUE, return mean, sd se predictors. FALSE, return predictions replicate. scaleDataset TRUE, scale whole dataset instead train set replicate. Optimize processing time predictions large rasters. NNmodel TRUE, return serialized model result. Use keras::unserialize_model() get model. DALEXexplainer TRUE, return explainer models DALEX::explain() function. work multisession future plans. variableResponse TRUE, return aggregated_profiles_explainer objects ingredients::partial_dependency() coefficients adjusted linear model. save_validateset save validateset (independent data used training). baseFilenameNN missing, save NN hdf5 format path iteration appended. filenameRasterPred missing, save predictions RasterBrick file. tempdirRaster path directory save temporal raster files. nCoresRaster number cores used parallelized raster cores. Use half available cores default. verbose > 0, print state passed keras functions ... extra parameters future.apply::future_replicate() ingredients::feature_importance().","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/pipe_keras_timeseries.html","id":null,"dir":"Reference","previous_headings":"","what":"Neural network model with keras — pipe_keras_timeseries","title":"Neural network model with keras — pipe_keras_timeseries","text":"Neural network model keras","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/pipe_keras_timeseries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Neural network model with keras — pipe_keras_timeseries","text":"","code":"pipe_keras_timeseries(   df,   predInput = NULL,   responseVars = 1,   caseClass = NULL,   idVars = character(),   weight = \"class\",   timevar = NULL,   responseTime = \"LAST\",   regex_time = \".+\",   staticVars = NULL,   crossValStrategy = c(\"Kfold\", \"bootstrap\"),   k = 5,   replicates = 10,   crossValRatio = c(train = 0.6, test = 0.2, validate = 0.2),   hidden_shape.RNN = c(32, 32),   hidden_shape.static = c(32, 32),   hidden_shape.main = 32,   epochs = 500,   maskNA = NULL,   batch_size = \"all\",   repVi = 5,   perm_dim = 2:3,   comb_dims = FALSE,   summarizePred = TRUE,   scaleDataset = FALSE,   NNmodel = FALSE,   DALEXexplainer = FALSE,   variableResponse = FALSE,   save_validateset = FALSE,   baseFilenameNN = NULL,   filenameRasterPred = NULL,   tempdirRaster = NULL,   nCoresRaster = parallel::detectCores()%/%2,   verbose = 0,   ... )"},{"path":"https://jmaspons.github.io/MLTools/reference/pipe_keras_timeseries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Neural network model with keras — pipe_keras_timeseries","text":"df data.frame data long format (time variable timevar column). predInput data.frame input variables make predictions. columns names must match names df columns. responseVars response variables column names indexes df wide format (eg. respVar_time). caseClass class samples used weight cases. Column names indexes df, vector class rows df. idVars id column names indexes df. unique identifier row wide format, otherwise, values averaged. weight Optional array length nrow(df), containing weights apply model's loss sample. timevar column name variable containing time. responseTime timevar value used response var responseVars default \"LAST\" last timestep available (max(df[, timevar])). regex_time regular expression matching timevar values format. staticVars predictor variables column names indexes df indicating fixed vars change time. crossValStrategy Kfold bootstrap. k number data partitions crossValStrategy=\"Kfold\". replicates number replicates crossValStrategy=\"bootstrap\" crossValStrategy=\"Kfold\" (replicates * k-1, 1 fold validation). crossValRatio Proportion dataset used train, test validate model crossValStrategy=\"bootstrap\". Default c(train=0.6, test=0.2, validate=0.2). one value, taken train proportion test set used validation. hidden_shape.RNN number neurons hidden layers Recursive Neural Network model (time series data). Can vector values hidden layer. hidden_shape.static number neurons hidden layers densely connected neural network model (static data). Can vector values hidden layer. hidden_shape.main number neurons hidden layers densely connected neural network model connecting static time series data. Can vector values hidden layer. epochs parameter keras::fit(). maskNA value assign NAs scaling passed keras::layer_masking(). batch_size fit predict functions. bigger better fits available memory. Integer \"\". repVi replicates permutations calculate importance variables. 0 avoid calculating variable importance. perm_dim dimension perform permutations calculate importance variables (data dimensions [case, time, variable]). perm_dim = 2:3, calculates importance combination 2nd 3rd dimensions. comb_dims variable importance calculations, TRUE, permutations combination levels variables 2nd 3rd dimensions input data 3 dimensions. default FALSE. summarizePred TRUE, return mean, sd se predictors. FALSE, return predictions replicate. scaleDataset TRUE, scale whole dataset instead train set replicate. Optimize processing time predictions large rasters. NNmodel TRUE, return serialized model result. DALEXexplainer TRUE, return explainer models DALEX::explain() function. work multisession future plans. variableResponse TRUE, return aggregated_profiles_explainer object ingredients::partial_dependency() coefficients adjusted linear model. save_validateset save validateset (independent data used training). baseFilenameNN missing, save NN hdf5 format path iteration appended. filenameRasterPred missing, save predictions RasterBrick file. tempdirRaster path directory save temporal raster files. nCoresRaster number cores used parallelized raster cores. Use half available cores default. verbose > 0, print state passed keras functions ... extra parameters future.apply::future_replicate()  ingredients::feature_importance().","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/pipe_randomForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Random forest model with randomForest — pipe_randomForest","title":"Random forest model with randomForest — pipe_randomForest","text":"Random forest model randomForest","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/pipe_randomForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random forest model with randomForest — pipe_randomForest","text":"","code":"pipe_randomForest(   df,   predInput = NULL,   responseVar = 1,   caseClass = NULL,   idVars = character(),   weight = \"class\",   crossValStrategy = c(\"Kfold\", \"bootstrap\"),   k = 5,   replicates = 10,   crossValRatio = c(train = 0.6, test = 0.2, validate = 0.2),   ntree = 500,   importance = TRUE,   shap = TRUE,   aggregate_shap = TRUE,   repVi = 5,   summarizePred = TRUE,   scaleDataset = FALSE,   RFmodel = FALSE,   DALEXexplainer = FALSE,   variableResponse = FALSE,   save_validateset = FALSE,   filenameRasterPred = NULL,   tempdirRaster = NULL,   nCoresRaster = parallel::detectCores()%/%2,   verbose = 0,   ... )"},{"path":"https://jmaspons.github.io/MLTools/reference/pipe_randomForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random forest model with randomForest — pipe_randomForest","text":"df data.frame data. predInput data.frame Raster input variables model columns layers. columns layer names must match names df columns. responseVar response variable column name index df. caseClass class samples used weight cases. Column names indexes df, vector class rows df. idVars id column names indexes df. columns used training. weight Optional array length nrow(df), containing weights apply model's loss sample. crossValStrategy Kfold bootstrap. k number data partitions crossValStrategy=\"Kfold\". replicates number replicates crossValStrategy=\"bootstrap\" crossValStrategy=\"Kfold\" (replicates * k-1, 1 fold validation). crossValRatio Proportion dataset used train, test validate model crossValStrategy=\"bootstrap\". Default c(train=0.6, test=0.2, validate=0.2). one value, taken train proportion test set used validation. ntree Number trees grow. importance parameter randomForest::randomForest() indicating importance predictors assessed. shap TRUE, return SHAP values shapviz::shapviz() objects. aggregate_shap TRUE, shap also TRUE, aggregate SHAP replicates. repVi replicates permutations calculate importance variables. 0 avoid calculating variable importance. summarizePred TRUE, return mean, sd se predictors. FALSE, return predictions replicate. scaleDataset TRUE, scale whole dataset instead train set replicate. Optimize processing time predictions large rasters. RFmodel TRUE, return model result. DALEXexplainer TRUE, return explainer models DALEX::explain() function. work multisession future plans. variableResponse TRUE, return aggregated_profiles_explainer object ingredients::partial_dependency() coefficients adjusted linear model. save_validateset save validateset (independent data used training). filenameRasterPred missing, save predictions RasterBrick file. tempdirRaster path directory save temporal raster files. nCoresRaster number cores used parallelized raster cores. Use half available cores default. verbose > 0, print state passed randomForest functions ... extra parameters randomForest::randomForest(), future.apply::future_replicate() ingredients::feature_importance().","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/pipe_xgboost.html","id":null,"dir":"Reference","previous_headings":"","what":"eXtrem Gradient Boosted models — pipe_xgboost","title":"eXtrem Gradient Boosted models — pipe_xgboost","text":"eXtrem Gradient Boosted models","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/pipe_xgboost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"eXtrem Gradient Boosted models — pipe_xgboost","text":"","code":"pipe_xgboost(   df,   predInput = NULL,   responseVars = 1,   caseClass = NULL,   idVars = character(),   weight = \"class\",   crossValStrategy = c(\"Kfold\", \"bootstrap\"),   k = 5,   replicates = 10,   crossValRatio = c(train = 0.6, test = 0.2, validate = 0.2),   params = list(),   nrounds = 5,   shap = TRUE,   aggregate_shap = TRUE,   repVi = 5,   summarizePred = TRUE,   scaleDataset = FALSE,   XGBmodel = FALSE,   DALEXexplainer = FALSE,   variableResponse = FALSE,   save_validateset = FALSE,   baseFilenameXDG = NULL,   filenameRasterPred = NULL,   tempdirRaster = NULL,   nCoresRaster = parallel::detectCores()%/%2,   verbose = 0,   ... )"},{"path":"https://jmaspons.github.io/MLTools/reference/pipe_xgboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"eXtrem Gradient Boosted models — pipe_xgboost","text":"df data.frame data. predInput data.frame Raster input variables model columns layers. columns layer names must match names df columns. responseVars response variables column names indexes df. caseClass class samples used weight cases. Column names indexes df, vector class rows df. idVars id column names indexes df. columns used training. weight Optional array length nrow(df), containing weights apply model's loss sample. crossValStrategy Kfold bootstrap. k number data partitions crossValStrategy=\"Kfold\". replicates number replicates crossValStrategy=\"bootstrap\" crossValStrategy=\"Kfold\" (replicates * k-1, 1 fold validation). crossValRatio proportion dataset used train, test validate model crossValStrategy=\"bootstrap\". Default c(train=0.6, test=0.2, validate=0.2). one value, taken train proportion test set used validation. params list parameters xgboost::xgb.train(). complete list parameters available online documentation. nrounds max number boosting iterations. shap TRUE, return SHAP values shapviz::shapviz() objects. aggregate_shap TRUE, shap also TRUE, aggregate SHAP replicates. repVi replicates permutations calculate importance variables. 0 avoid calculating variable importance. summarizePred TRUE, return mean, sd se predictors. FALSE, return predictions replicate. scaleDataset TRUE, scale whole dataset instead train set replicate. Optimize processing time predictions large rasters. XGBmodel TRUE, return model result. DALEXexplainer TRUE, return explainer models DALEX::explain() function. work multisession future plans. variableResponse TRUE, return aggregated_profiles_explainer object ingredients::partial_dependency() coefficients adjusted linear model. save_validateset save validateset (independent data used training). baseFilenameXDG missing, save NN hdf5 format path iteration appended. filenameRasterPred missing, save predictions RasterBrick file. tempdirRaster path directory save temporal raster files. nCoresRaster number cores used parallelized raster cores. Use half available cores default. verbose > 0, print state. bigger information printed. ... extra parameters xgboost::xgb.train(), future.apply::future_replicate() ingredients::feature_importance().","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/plotVI.pipe_result.html","id":null,"dir":"Reference","previous_headings":"","what":"plotVI.pipe_result — plotVI.pipe_result","title":"plotVI.pipe_result — plotVI.pipe_result","text":"plotVI.pipe_result","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/plotVI.pipe_result.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plotVI.pipe_result — plotVI.pipe_result","text":"","code":"plotVI.pipe_result(   res,   vi = c(\"ratio\", \"diff\", \"raw\"),   dispersion = c(\"sd\", \"se\", \"ci\") )"},{"path":"https://jmaspons.github.io/MLTools/reference/plotVI.pipe_result.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plotVI.pipe_result — plotVI.pipe_result","text":"res pipe_result object. vi output type . ratio diff, performance compared full model without permutation. dispersion sd, se ci. Metric dispersion bars","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/predict_keras.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict with keras — predict_keras","title":"Predict with keras — predict_keras","text":"Predict keras","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/predict_keras.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict with keras — predict_keras","text":"","code":"predict_keras(   modelNN,   predInput,   maskNA = NULL,   scaleInput = FALSE,   col_means_train,   col_stddevs_train,   batch_size = NULL,   filename = \"\",   tempdirRaster = NULL,   nCoresRaster = 2 )"},{"path":"https://jmaspons.github.io/MLTools/reference/predict_keras.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict with keras — predict_keras","text":"modelNN keras::keras() model. predInput data.frame raster colnames layer names matching expected input modelRF. maskNA value assign NAs scaling passed keras::layer_masking(). scaleInput TRUE, scale predInput col_means_train col col_stddevs_train. col_means_train original mean predInput columns. col_stddevs_train original sd predInput columns. batch_size fit predict functions. bigger better fits available memory. Integer \"\". filename file write raster predictions. tempdirRaster path directory save temporal raster files. nCoresRaster number cores used parallelized raster cores. Use half available cores default.","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/predict_randomForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict with randomForest — predict_randomForest","title":"Predict with randomForest — predict_randomForest","text":"Predict randomForest","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/predict_randomForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict with randomForest — predict_randomForest","text":"","code":"predict_randomForest(   modelRF,   predInput,   scaleInput = FALSE,   col_means_train,   col_stddevs_train,   filename = \"\",   tempdirRaster = NULL,   nCoresRaster = 2 )"},{"path":"https://jmaspons.github.io/MLTools/reference/predict_randomForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict with randomForest — predict_randomForest","text":"modelRF randomForest::randomForest() model. predInput data.frame raster colnames layer names matching expected input modelRF. scaleInput TRUE, scale predInput col_means_train col col_stddevs_train. col_means_train original mean predInput columns. col_stddevs_train original sd predInput columns. filename file write raster predictions. tempdirRaster path directory save temporal raster files. nCoresRaster number cores used parallelized raster cores. Use half available cores default.","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/predict_xgboost.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict with xgboost — predict_xgboost","title":"Predict with xgboost — predict_xgboost","text":"Predict xgboost","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/predict_xgboost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict with xgboost — predict_xgboost","text":"","code":"predict_xgboost(   modelXGB,   predInput,   scaleInput = FALSE,   col_means_train,   col_stddevs_train,   filename = \"\",   tempdirRaster = NULL,   nCoresRaster = 2 )"},{"path":"https://jmaspons.github.io/MLTools/reference/predict_xgboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict with xgboost — predict_xgboost","text":"modelXGB xgboost::xgboost() model. predInput data.frame raster colnames layer names matching expected input modelRF. scaleInput TRUE, scale predInput col_means_train col col_stddevs_train. col_means_train original mean predInput columns. col_stddevs_train original sd predInput columns. filename file write raster predictions. tempdirRaster path directory save temporal raster files. nCoresRaster number cores used parallelized raster cores. Use half available cores default.","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/summary.pipe_result.html","id":null,"dir":"Reference","previous_headings":"","what":"summary.pipe_result — summary.pipe_result","title":"summary.pipe_result — summary.pipe_result","text":"summary.pipe_result","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/summary.pipe_result.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary.pipe_result — summary.pipe_result","text":"","code":"# S3 method for pipe_result summary(object, ...)"},{"path":"https://jmaspons.github.io/MLTools/reference/summary.pipe_result.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary.pipe_result — summary.pipe_result","text":"object pipe_result object. ... parameters summarize_pred.Raster function.","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/transformTS.html","id":null,"dir":"Reference","previous_headings":"","what":"Time series format transformations — transformTS","title":"Time series format transformations — transformTS","text":"data wide format, name time varying columns follow pattern var_time.","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/transformTS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Time series format transformations — transformTS","text":"","code":"longToWide.ts(d, timevar, idCols = NULL)  wideToLong.ts(d, timevar, vars, idCols = NULL, regex_time = \".+\")  wideTo3Darray.ts(d, vars, idCols = NULL)  longTo3Darray.ts(d, timevar, idCols = NULL)"},{"path":"https://jmaspons.github.io/MLTools/reference/transformTS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Time series format transformations — transformTS","text":"d data.frame matrix. timevar column name time. idCols ids time varying columns. Works column names indexes. vars time varying variables long format (ie. without _time part wide format). regex_time regular expression time part variable names data wide format.","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/transformTS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Time series format transformations — transformTS","text":"data wide [var_time columns], long [time + vars columns] 3D array [samples, time, vars] format.","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/variableImportance.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable importance by permutations on predictors — variableImportance","title":"Variable importance by permutations on predictors — variableImportance","text":"Variable importance permutations predictors Variable importance permutations predictors Variable importance permutations predictors","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/variableImportance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable importance by permutations on predictors — variableImportance","text":"","code":"variableImportance(   model,   data,   y,   repVi = 5,   variable_groups = NULL,   perm_dim = NULL,   comb_dims = FALSE,   ... )  variableImportance(   model,   data,   y,   repVi = 5,   variable_groups = NULL,   perm_dim = NULL,   comb_dims = FALSE,   ... )  variableImportance(   model,   data,   y,   repVi = 5,   variable_groups = NULL,   perm_dim = NULL,   comb_dims = FALSE,   ... )"},{"path":"https://jmaspons.github.io/MLTools/reference/variableImportance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable importance by permutations on predictors — variableImportance","text":"model model use predictions. data input data permute use predictions. y response data corresponding data features. repVi replicates permutations calculate importance variables. 0 avoid calculating variable importance. variable_groups list variables join calculating variable importance permuting time. perm_dim dimension perform permutations calculate importance variables (data dimensions [case, time, variable]). perm_dim = 2:3, calculates importance combination 2nd 3rd dimensions. comb_dims variable importance calculations, TRUE, permutations combination levels variables 2nd 3rd dimensions input data 3 dimensions. default FALSE. ...","code":""},{"path":"https://jmaspons.github.io/MLTools/reference/variableImportance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Variable importance by permutations on predictors — variableImportance","text":"See ingredients::feature_importance(). function also works multiinput 3d data.","code":""}]
